# -*- coding: utf-8 -*-
"""lassifying_Sleep_Disorders.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1Gf6FsGgSHJJoH8eOg9VBWrzyxst7ccDw
"""



#Import the needed libraries
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
import warnings
warnings.filterwarnings("ignore")
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler, OneHotEncoder
from sklearn.compose import ColumnTransformer
from sklearn.pipeline import Pipeline
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Conv1D, MaxPooling1D, Flatten, Dense, Dropout, GRU, Bidirectional, LSTM
from tensorflow.keras.utils import to_categorical
from tensorflow.keras.optimizers import Adam
from tensorflow.keras.callbacks import LearningRateScheduler
from sklearn.naive_bayes import GaussianNB
from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import accuracy_score, classification_report, confusion_matrix
from sklearn.tree import DecisionTreeClassifier
np.random.seed(42)

from google.colab import drive
drive.mount('/content/drive')

#Read and check the dataset
sleep = pd.read_csv("/content/drive/MyDrive/Colab Notebooks (1)/Sleep Health and Lifestyle Dataset.csv")
sleep.head()

#The missing values probably mean that the person doesn't have a sickness
#Replace NaN-s with No Disorder
sleep["Sleep Disorder"] = sleep["Sleep Disorder"].fillna("No Disorder")

#Separate the Blood Pressure values to Systolic and Diastolic
sleep[["Systoli", "Diastolic"]] = sleep["Blood Pressure"].str.split("/", expand = True).astype(int)

#Drop the Id column
sleep.drop("Person ID", axis = 1, inplace = True)

#Lets have a quick look on the basic correlations
corr = sleep.corr(numeric_only = True)
corr

"""#** EDA**"""

#Sleep disorder based on gender
sns.countplot(x = "Gender", hue = "Sleep Disorder", data = sleep);

#Sleep disorder based on occupation
sns.countplot(y = "Occupation", hue = "Sleep Disorder", data = sleep);

#Let's see, which occupations are more threatened by sleeping disorders based also on gender, to check for imbalances
sns.catplot(y="Occupation", hue="Sleep Disorder",col= "Gender",kind = "count", data=sleep)
plt.show()

#It seems there is a strong cofounding factor of occupation. We can see that jobs which probably pay less, like nurse are done by females
#while higher paying jobs. like doctor, by men.
#Lets see the distribution in gender if we don't include the nurses
no_nurse = sleep.loc[sleep["Occupation"] != "Nurse"]
sns.countplot(x = "Gender", hue = "Sleep Disorder", data = no_nurse);

#Create boxplots about the sleep length
sns.boxplot(x = "Sleep Disorder",  y = "Sleep Duration", data = sleep);

#Let's find out if there are some other variables, which are pushing nurses to the direction of sleep apnea
#First make another correlation, but now numeric values for sleep disorders

sleep_dummies = pd.get_dummies(sleep["Sleep Disorder"])
sleep_new = sleep.drop("Sleep Disorder", axis = 1)
sleep_disorders = pd.concat([sleep_new, sleep_dummies], axis = 1)

#Make a heatmap, see what variables are connected to which outcome
corr_2 = sleep_disorders.corr(numeric_only = True)
plt.figure(figsize = (8,8))
sns.heatmap(corr_2.iloc[:-3, -3:], annot = True, cmap='RdBu_r');

print(sleep_disorders.columns)

no_normal = sleep_disorders.drop("None".strip(), axis=1)
if "No Disorder" in sleep_disorders.columns:
    no_normal = sleep_disorders.drop("No Disorder", axis=1)
else:
    print("Column 'No Disorder' not found.")

correlation_4 = no_normal.corr(numeric_only = True)
sns.heatmap(correlation_4.iloc[:-2, -2:], annot = True, cmap='RdBu_r');

"""# **Train Test Set Value**"""

#Make a train and a test set
from sklearn.model_selection import train_test_split
y = sleep_disorders[["None", "Insomnia", "Sleep Apnea"]]
X = sleep_disorders.drop(["Insomnia", "None", "Sleep Apnea", "Blood Pressure"], axis = 1)
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.25, random_state = 42)

#Generating dummies and standardize
X_train = pd.get_dummies(X_train, columns=["Gender", "Occupation"])
X_test = pd.get_dummies(X_test, columns=["Gender", "Occupation"])
X_test = X_test.reindex(columns=X_train.columns, fill_value=0)

# Make a train and a test set
y = sleep_disorders[["None", "Insomnia", "Sleep Apnea"]]
X = sleep_disorders.drop(["Insomnia", "None", "Sleep Apnea", "Blood Pressure"], axis=1)
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, random_state=42)

# Identify numeric and categorical columns
numeric_features = X_train.select_dtypes(include=['float64', 'int64']).columns
categorical_features = X_train.select_dtypes(include=['object']).columns

# Create transformers for numeric and categorical features
numeric_transformer = StandardScaler()
categorical_transformer = OneHotEncoder(handle_unknown='ignore')

# Create preprocessor using ColumnTransformer
preprocessor = ColumnTransformer(
    transformers=[
        ('num', numeric_transformer, numeric_features),
        ('cat', categorical_transformer, categorical_features)
    ])

# Create a pipeline with preprocessing and scaling
pipeline = Pipeline(steps=[('preprocessor', preprocessor),
                             ('scaler', StandardScaler())])

# Fit and transform the training data
X_train_scaled = pipeline.fit_transform(X_train)

# Transform the test data using the same pipeline
X_test_scaled = pipeline.transform(X_test)

#Seeing the results with a KNN model
from sklearn.model_selection import GridSearchCV
from sklearn.neighbors import KNeighborsClassifier

# Define the parameter grid
param_grid = {'n_neighbors': [1, 3, 5, 6, 7, 9, 11],
              'weights': ['uniform', 'distance']}
# Create a KNN classifier object
knn = KNeighborsClassifier()

# Create a GridSearchCV object
grid_search = GridSearchCV(knn, param_grid, cv=5)

# Fit the GridSearchCV object to the data
grid_search.fit(X_train_scaled, y_train)

# Print the best hyperparameters
print(grid_search.best_params_)
# Train the classifier using your training data
knn.fit(X_train_scaled, y_train)

# Make predictions on your test data
y_pred = knn.predict(X_test_scaled)

from sklearn import metrics
result = metrics.accuracy_score(y_pred,y_test)
print(result)
classification_rep = metrics.classification_report(y_test, y_pred)
print(classification_rep)

#Create the labels for the matrix
disease_labels = ["Insomnia", "No Disorder", "Sleep Apnea"]
#Create a confusion matrix
cm = metrics.confusion_matrix(np.asarray(y_test).argmax(axis=1), np.asarray(y_pred).argmax(axis=1))
# Create a figure object
fig = plt.figure()
# Add an ax object to the figure
ax = fig.add_subplot(111)
#Create the visualisation
ax.set_xlabel('Predicted labels');
ax.set_ylabel('True labels');
ax.set_title('Confusion Matrix');
sns.heatmap(cm, annot=True, fmt='g', xticklabels=disease_labels, yticklabels=disease_labels);

"""# **DecisionTreeClassifier**"""

# Make a train and a test set
y = sleep_disorders[["Insomnia", "Sleep Apnea", 'New_Sleep Disorder']]
X = sleep_disorders.drop(["Insomnia", "None", "Sleep Apnea", "Blood Pressure", 'New_Sleep Disorder'], axis=1)
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, random_state=42)

# Identify numeric and categorical columns
numeric_features = X_train.select_dtypes(include=['float64', 'int64']).columns
categorical_features = X_train.select_dtypes(include=['object']).columns



# Create transformers for numeric and categorical features
numeric_transformer = StandardScaler()
categorical_transformer = OneHotEncoder(handle_unknown='ignore')


# Create preprocessor using ColumnTransformer
preprocessor = ColumnTransformer(
    transformers=[
        ('num', numeric_transformer, numeric_features),
        ('cat', categorical_transformer, categorical_features)
    ])

# Create a pipeline with preprocessing, scaling, and decision tree
pipeline_dtc = Pipeline(steps=[('preprocessor', preprocessor),
                                ('classifier', DecisionTreeClassifier(random_state=42))])

# Fit the pipeline to the training data
pipeline_dtc.fit(X_train, y_train)

# Predict on the test set
y_pred_dtc = pipeline_dtc.predict(X_test)

# Iterate over each column and print classification report
for i, column in enumerate(y.columns):
    y_true_column = y_test.iloc[:, i]
    y_pred_column = y_pred_dtc[:, i]

    print(f"Classification Report for {column}:")
    print(classification_report(y_true_column, y_pred_column))

    # Print confusion matrix
    conf_matrix_column = confusion_matrix(y_true_column, y_pred_column)



# Visualize overall confusion matrix with heatmap
conf_matrix_overall = confusion_matrix(np.argmax(y_test.values, axis=1), np.argmax(y_pred_dtc, axis=1))
plt.figure(figsize=(8, 6))
sns.heatmap(conf_matrix_overall, annot=True, fmt='d', cmap='Blues', xticklabels=['None', 'Sleep Apnea', 'Insomnia'], yticklabels=['None', 'Sleep Apnea', 'Insomnia'])

plt.title("Overall Confusion Matrix")
plt.xlabel("Predicted Labels")
plt.ylabel("True Labels")
plt.show()

"""### RF and GN"""

sleep_disorders = pd.read_csv("/content/drive/MyDrive/Colab Notebooks (1)/Sleep Health and Lifestyle Dataset.csv")
# Extract features and target variable
y = sleep_disorders["New_Sleep Disorder"]
X = sleep_disorders.drop(["Person ID", "Sleep Disorder", "New_Sleep Disorder", "Blood Pressure"], axis=1)

# Identify numeric and categorical columns
numeric_features = X.select_dtypes(include=['float64', 'int64']).columns
categorical_features = X.select_dtypes(include=['object']).columns

# Create transformers for numeric and categorical features
numeric_transformer = StandardScaler()
categorical_transformer = OneHotEncoder(handle_unknown='ignore')

# Create preprocessor using ColumnTransformer
preprocessor = ColumnTransformer(
    transformers=[
        ('num', numeric_transformer, numeric_features),
        ('cat', categorical_transformer, categorical_features)
    ])

# Create a pipeline with preprocessing
pipeline = Pipeline(steps=[('preprocessor', preprocessor)])

# Transform the data
X_transformed = pipeline.fit_transform(X)

# Split the data into train and test sets
X_train, X_test, y_train, y_test = train_test_split(X_transformed, y, test_size=0.25, random_state=42)

# Build and train the Random Forest classifier with hyperparameter tuning
rf_model = RandomForestClassifier(n_estimators=100, max_depth=10, random_state=42)
rf_model.fit(X_train, y_train)

# Build and train the Gaussian Naive Bayes model
nb_model = GaussianNB()
nb_model.fit(X_train, y_train)

# Make predictions on the test set for both models
y_pred_rf = rf_model.predict(X_test)
y_pred_nb = nb_model.predict(X_test)

# Calculate accuracy for both models
accuracy_rf = accuracy_score(y_test, y_pred_rf)
accuracy_nb = accuracy_score(y_test, y_pred_nb)

print(f"Random Forest Accuracy: {accuracy_rf}")
print(f"Gaussian Naive Bayes Accuracy: {accuracy_nb}")

# Print additional metrics for Random Forest
classification_report_rf = classification_report(y_test, y_pred_rf)
conf_matrix_rf = confusion_matrix(y_test, y_pred_rf)

print("Random Forest Metrics:")
print(f"Classification Report:\n{classification_report_rf}")


# Print additional metrics for Gaussian Naive Bayes
classification_report_nb = classification_report(y_test, y_pred_nb)
conf_matrix_nb = confusion_matrix(y_test, y_pred_nb)

print("Gaussian Naive Bayes Metrics:")
print(f"Classification Report:\n{classification_report_nb}")

# Plot confusion matrix for Random Forest
plt.figure(figsize=(8, 6))
sns.heatmap(conf_matrix_rf, annot=True, fmt='d', cmap='Blues', xticklabels=['None', 'Sleep Apnea', 'Insomnia'], yticklabels=['None', 'Sleep Apnea', 'Insomnia'])
plt.title('Random Forest Confusion Matrix')
plt.xlabel('Predicted')
plt.ylabel('Actual')
plt.show()

# Plot confusion matrix for Gaussian Naive Bayes
plt.figure(figsize=(8, 6))
sns.heatmap(conf_matrix_nb, annot=True, fmt='d', cmap='Blues', xticklabels=['None', 'Sleep Apnea', 'Insomnia'], yticklabels=['None', 'Sleep Apnea', 'Insomnia'])
plt.title('Gaussian Naive Bayes Confusion Matrix')
plt.xlabel('Predicted')
plt.ylabel('Actual')
plt.show()

"""### CNN MODEL"""

X_scaled = pipeline.fit_transform(X)

# Reshape the data for CNN input
X_cnn = X_scaled.reshape((X_scaled.shape[0], X_scaled.shape[1], 1))

# One-hot encode the target variable with 3 classes
y_encoded = to_categorical(y, num_classes=3)

# Split the data into train and test sets
X_train, X_test, y_train, y_test = train_test_split(X_cnn, y_encoded, test_size=0.25, random_state=42)
# Build the CNN model
cnn_model = Sequential()
cnn_model.add(Conv1D(filters=64, kernel_size=3, activation='relu', input_shape=(X_cnn.shape[1], 1)))
cnn_model.add(MaxPooling1D(pool_size=1))
cnn_model.add(Flatten())
cnn_model.add(Dense(128, activation='relu'))  # Increase the number of neurons
cnn_model.add(Dropout(0.8))
cnn_model.add(Dense(3, activation='softmax'))

# Compile the CNN model with a lower learning rate
cnn_model.compile(optimizer=Adam(learning_rate=0.0005), loss='categorical_crossentropy', metrics=['accuracy'])

# Train the CNN model with more epochs
history2 = cnn_model.fit(X_train, y_train, epochs=30, batch_size=32, validation_split=0.3)

plt.figure(figsize=(12, 6))

# Plot training & validation accuracy values
plt.subplot(1, 2, 1)
plt.plot(history2.history['accuracy'])
plt.plot(history2.history['val_accuracy'])
plt.title('Model Accuracy')
plt.xlabel('Epoch')
plt.ylabel('Accuracy')
plt.legend(['Train', 'Validation'], loc='upper left')

# Plot training & validation loss values
plt.subplot(1, 2, 2)
plt.plot(history2.history['loss'])
plt.plot(history2.history['val_loss'])
plt.title('Model Loss')
plt.xlabel('Epoch')
plt.ylabel('Loss')
plt.legend(['Train', 'Validation'], loc='upper left')

plt.tight_layout()
plt.show()

# Evaluate the CNN model on test data
y_pred_cnn = np.argmax(cnn_model.predict(X_test), axis=1)
y_test_categorical = np.argmax(y_test, axis=1)

# Print metrics for CNN
accuracy_cnn = np.sum(y_pred_cnn == y_test_categorical) / len(y_test_categorical)
classification_report_cnn = classification_report(y_test_categorical, y_pred_cnn)
conf_matrix_cnn = confusion_matrix(y_test_categorical, y_pred_cnn)

print("CNN Model Metrics:")
print(f"Accuracy: {accuracy_cnn}")
print(f"Classification Report:\n{classification_report_cnn}")

# Plot the confusion matrix for CNN
plt.figure(figsize=(8, 6))
sns.heatmap(conf_matrix_cnn, annot=True, fmt='d', cmap='Blues', xticklabels=['None', 'Sleep Apnea', 'Insomnia'], yticklabels=['None', 'Sleep Apnea', 'Insomnia'])
plt.title('CNN Confusion Matrix')
plt.xlabel('Predicted')
plt.ylabel('Actual')
plt.show()

"""# CNN+BI-LSTM"""

# Build the CNN + Bidirectional LSTM model
cnn_lstm_model = Sequential()
cnn_lstm_model.add(Conv1D(filters=128, kernel_size=2, activation='relu', input_shape=(X_cnn.shape[1], 1)))
cnn_lstm_model.add(MaxPooling1D(pool_size=2))
cnn_lstm_model.add(Bidirectional(LSTM(64, activation='relu', return_sequences=True)))
cnn_lstm_model.add(Bidirectional(LSTM(32, activation='relu')))
cnn_lstm_model.add(Flatten())
cnn_lstm_model.add(Dense(128, activation='relu'))
cnn_lstm_model.add(Dropout(0.5))
cnn_lstm_model.add(Dense(64, activation='relu'))
cnn_lstm_model.add(Dense(3, activation='softmax'))

# Compile the CNN + Bidirectional LSTM model
cnn_lstm_model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])

# Train the CNN + Bidirectional LSTM model
history1=cnn_lstm_model.fit(X_train, y_train, epochs=30, batch_size=8, validation_split=0.2)

plt.figure(figsize=(12, 6))

# Plot training & validation accuracy values
plt.subplot(1, 2, 1)
plt.plot(history1.history['accuracy'])
plt.plot(history1.history['val_accuracy'])
plt.title('Model Accuracy')
plt.xlabel('Epoch')
plt.ylabel('Accuracy')
plt.legend(['Train', 'Validation'], loc='upper left')

# Plot training & validation loss values
plt.subplot(1, 2, 2)
plt.plot(history1.history['loss'])
plt.plot(history1.history['val_loss'])
plt.title('Model Loss')
plt.xlabel('Epoch')
plt.ylabel('Loss')
plt.legend(['Train', 'Validation'], loc='upper left')

plt.tight_layout()
plt.show()

# Evaluate the CNN + Bidirectional LSTM model on test data
y_pred_cnn_lstm = np.argmax(cnn_lstm_model.predict(X_test), axis=1)

# Print metrics for CNN + Bidirectional LSTM
accuracy_cnn_lstm = np.sum(y_pred_cnn_lstm == y_test_categorical) / len(y_test_categorical)
classification_report_cnn_lstm = classification_report(y_test_categorical, y_pred_cnn_lstm)
conf_matrix_cnn_lstm = confusion_matrix(y_test_categorical, y_pred_cnn_lstm)

print("CNN + Bidirectional LSTM Model Metrics:")
print(f"Accuracy: {accuracy_cnn_lstm}")
print(f"Classification Report:\n{classification_report_cnn_lstm}")

# Plot the confusion matrix for CNN + Bidirectional LSTM
plt.figure(figsize=(8, 6))
sns.heatmap(conf_matrix_cnn_lstm, annot=True, fmt='d', cmap='Blues', xticklabels=['None', 'Sleep Apnea', 'Insomnia'], yticklabels=['None', 'Sleep Apnea', 'Insomnia'])
plt.title('CNN + Bidirectional LSTM Confusion Matrix')
plt.xlabel('Predicted')
plt.ylabel('Actual')
plt.show()

from sklearn.metrics import roc_curve, auc
from keras.utils import to_categorical
import matplotlib.pyplot as plt

# Get predicted probabilities for each class
y_pred_prob_cnn_lstm = cnn_lstm_model.predict(X_test)

# Compute ROC curve and AUC for each class
fpr = dict()
tpr = dict()
roc_auc = dict()

n_classes = 3  # Replace this with the actual number of classes

for i in range(n_classes):
    fpr[i], tpr[i], _ = roc_curve(y_test[:, i], y_pred_prob_cnn_lstm[:, i])
    roc_auc[i] = auc(fpr[i], tpr[i])

# Plot ROC curves
plt.figure(figsize=(8, 8))

for i in range(n_classes):
    plt.plot(fpr[i], tpr[i], label=f'Class {i} (AUC = {roc_auc[i]:.2f})')

plt.plot([0, 1], [0, 1], 'k--', lw=2)
plt.xlim([0.0, 1.0])
plt.ylim([0.0, 1.05])
plt.xlabel('False Positive Rate')
plt.ylabel('True Positive Rate')
plt.title('ROC Curve for CNN + Bidirectional LSTM')
plt.legend(loc='lower right')
plt.show()

"""# Accuracy Scores Comparison"""

import matplotlib.pyplot as plt

# Accuracy scores
accuracy_scores = {"RF": 0.9224, "GNB": 0.8576, "CNN": 0.9136, "CNN+BiLSTM": 0.9304}

# Creating bar chart
plt.figure(figsize=(8, 6))
plt.bar(accuracy_scores.keys(), accuracy_scores.values(), color=['blue', 'orange', 'green', 'red'])
plt.title('Accuracy Scores Comparison')
plt.xlabel('Model')
plt.ylabel('Accuracy')
plt.ylim(0.8, 1.0)  # Set y-axis limits for better visualization
plt.show()

"""# extra"""